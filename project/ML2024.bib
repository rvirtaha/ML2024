@article{bartlettClassificationRejectOption2008,
  title = {Classification with a {{Reject Option}} Using a {{Hinge Loss}}},
  author = {Bartlett, Peter L and Wegkamp, Marten H},
  date = {2008-08},
  abstract = {We consider the problem of binary classification where the classifier can, for a particular cost, choose not to classify an observation. Just as in the conventional classification problem, minimization of the sample average of the cost is a difficult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efficiently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values.},
  langid = {english},
  file = {C:\Users\rvirt\Zotero\storage\3IQDW9PL\Bartlett ja Wegkamp - Classiﬁcation with a Reject Option using a Hinge Loss.pdf}
}

@online{ComputerVisionTensorFlow,
  title = {Computer Vision with {{TensorFlow}} | {{TensorFlow Core}}},
  url = {https://www.tensorflow.org/tutorials/images},
  urldate = {2024-09-20},
  langid = {english},
  file = {C:\Users\rvirt\Zotero\storage\6K39LQCM\images.html}
}

@online{ConvolutionalNeuralNetwork,
  title = {Convolutional {{Neural Network}} ({{CNN}}) | {{TensorFlow Core}}},
  url = {https://www.tensorflow.org/tutorials/images/cnn},
  urldate = {2024-10-08},
  langid = {english},
  organization = {TensorFlow},
  file = {C:\Users\rvirt\Zotero\storage\9DEX3BG4\cnn.html}
}

@online{ImportanceFeatureScaling,
  title = {Importance of {{Feature Scaling}}},
  url = {https://scikit-learn/stable/auto_examples/preprocessing/plot_scaling_importance.html},
  urldate = {2024-09-20},
  abstract = {Feature scaling through standardization, also called Z-score normalization, is an important preprocessing step for many machine learning algorithms. It involves rescaling each feature such that it ...},
  langid = {english},
  organization = {scikit-learn},
  file = {C:\Users\rvirt\Zotero\storage\N9XHW2KI\plot_scaling_importance.html}
}

@online{jeevaLossFunctionsNeural2023,
  title = {Loss {{Functions}} in {{Neural Networks}}},
  author = {Jeeva, Cathrine},
  date = {2023-05-09T00:00:00},
  url = {https://www.scaler.com/topics/loss-functions-in-neural-networks/},
  urldate = {2024-10-08},
  abstract = {This article covers an overview of loss Functions in Neural Networks in Deep Learning.},
  langid = {english},
  organization = {Scaler Topics},
  file = {C:\Users\rvirt\Zotero\storage\CZCQLPBR\loss-functions-in-neural-networks.html}
}

@article{josephOptimalRatioData2022,
  title = {Optimal {{Ratio}} for {{Data Splitting}}},
  author = {Joseph, V. Roshan},
  date = {2022-08},
  journaltitle = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  shortjournal = {Statistical Analysis},
  volume = {15},
  number = {4},
  eprint = {2202.03326},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {531--538},
  issn = {1932-1864, 1932-1872},
  doi = {10.1002/sam.11583},
  url = {http://arxiv.org/abs/2202.03326},
  urldate = {2024-09-19},
  abstract = {It is common to split a dataset into training and testing sets before fitting a statistical or machine learning model. However, there is no clear guidance on how much data should be used for training and testing. In this article we show that the optimal splitting ratio is \$\textbackslash sqrt\{p\}:1\$, where \$p\$ is the number of parameters in a linear regression model that explains the data well.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\rvirt\\Zotero\\storage\\AMMYGZFT\\Joseph - 2022 - Optimal Ratio for Data Splitting.pdf;C\:\\Users\\rvirt\\Zotero\\storage\\FPDY3D7U\\2202.html}
}

@book{jungMachineLearningBasics2022,
  title = {Machine {{Learning}}: {{The Basics}}},
  shorttitle = {Machine {{Learning}}},
  author = {Jung, Alexander},
  date = {2022},
  series = {Machine {{Learning}}: {{Foundations}}, {{Methodologies}}, and {{Applications}}},
  publisher = {Springer Nature Singapore},
  location = {Singapore},
  doi = {10.1007/978-981-16-8193-6},
  url = {https://link.springer.com/10.1007/978-981-16-8193-6},
  urldate = {2024-09-20},
  isbn = {9789811681929 9789811681936},
  langid = {english},
  file = {C:\Users\rvirt\Zotero\storage\24LREQCK\Jung - 2022 - Machine Learning The Basics.pdf}
}

@article{nobleWhatSupportVector2006,
  title = {What Is a Support Vector Machine?},
  author = {Noble, William S},
  date = {2006-12},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {24},
  number = {12},
  pages = {1565--1567},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt1206-1565},
  url = {https://www.nature.com/articles/nbt1206-1565},
  urldate = {2024-09-20},
  langid = {english},
  file = {C:\Users\rvirt\Zotero\storage\BGSV34YL\Noble - 2006 - What is a support vector machine.pdf}
}

@online{osheaIntroductionConvolutionalNeural2015,
  title = {An {{Introduction}} to {{Convolutional Neural Networks}}},
  author = {O'Shea, Keiron and Nash, Ryan},
  date = {2015-12-02},
  eprint = {1511.08458},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.08458},
  urldate = {2024-10-08},
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\rvirt\Zotero\storage\85J38V4Y\O'Shea ja Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf}
}

@online{PCA,
  title = {{{PCA}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.decomposition.PCA.html},
  urldate = {2024-10-09},
  abstract = {Gallery examples: Release Highlights for scikit-learn 1.5 Release Highlights for scikit-learn 1.4 A demo of K-Means clustering on the handwritten digits data Principal Component Regression vs Parti...},
  langid = {english},
  organization = {scikit-learn},
  file = {C:\Users\rvirt\Zotero\storage\Z3GFJFHC\sklearn.decomposition.PCA.html}
}

@report{unknownMachineLearningApproach2023,
  title = {A {{Machine Learning Approach}} to {{Classifying Bangla Handwritten Characters}}},
  author = {{unknown}},
  date = {2023-09},
  file = {C:\Users\rvirt\Zotero\storage\WADLHR9B\Bangla_Handwritten_Character_Classification-S2-1.pdf}
}

@article{woldPrincipalComponentAnalysis,
  title = {Principal {{Component Analysis}}},
  author = {Wold, Svante and Geladi, KIM ESBENSENand PAUL},
  langid = {english},
  file = {C:\Users\rvirt\Zotero\storage\YF72L5JJ\Wold ja Geladi - Principal Component Analysis.pdf}
}
